import streamlit as st
from PIL import Image
import torch.nn as nn
import numpy as np
import torch
import cv2
import pandas as pd
import os

from transformers import SegformerImageProcessor, AutoModelForSemanticSegmentation
from transformers import AutoImageProcessor, DetrForObjectDetection
# segmentation
processor_seg = SegformerImageProcessor.from_pretrained("mattmdjaga/segformer_b2_clothes")
model_seg = AutoModelForSemanticSegmentation.from_pretrained("mattmdjaga/segformer_b2_clothes")
#object detection
processor_obj = AutoImageProcessor.from_pretrained("facebook/detr-resnet-50")
model_obj = DetrForObjectDetection.from_pretrained("facebook/detr-resnet-50")


def center_image(image_path,width=700):
    st.markdown(
        f'<style>img {{ display: block; margin-left: auto; margin-right: auto; }} </style>',
        unsafe_allow_html=True
    )
    st.image(image_path,width = width)


### INTRO ###
st.header('ğŸ‘š ì˜¤ëŠ˜ ë­ì…ì§€?! ğŸ‘•')
st.markdown('ğŸ’¬ : ğŸš¨ **ì„¤ë§ˆ ë„ˆ ì§€ê¸ˆ.. ê·¸ë ‡ê²Œ ì…ê³  ë‚˜ê°€ê²Œ?** ğŸš¨')
st.markdown(' **íŒ¨ì…˜ì„¼ìŠ¤ê°€ 2% ë¶€ì¡±í•œ ë‹¹ì‹ ì„ ìœ„í•´ ì¤€ë¹„í–ˆìŠµë‹ˆë‹¤!** ì‚¬ì§„ ì´ë¯¸ì§€ë§Œ ì…ë ¥í•˜ë©´, ìš”ì¦˜ íŠ¸ë Œë””í•œ ìŠ¤íƒ€ì¼ê³¼ ì—¬ëŸ¬ë¶„ì˜ TPOë¥¼ ê³ ë ¤í•˜ì—¬ ì½”ë””ë¥¼ ì¶”ì²œí•´ë“œë¦½ë‹ˆë‹¤. ë¬´ì‹ ì‚¬ì™€ ì˜¨ë”ë£©ì˜ íŒ¨ì…”ë‹ˆìŠ¤íƒ€ë“¤ì˜ ì½”ë””ë¥¼ ì§€ê¸ˆ ë°”ë¡œ ì°¸ê³ í•´ë³´ì„¸ìš”! ')
center_image('./intro_img/fashionista.jpg')

st.markdown('--------------------------------------------------------------------------------------')
st.subheader('PROCESS')
center_image('./intro_img/process.png')
st.markdown('--------------------------------------------------------------------------------------')


## INPUT ###
st.subheader(' âœ… ì˜ë¥˜ ì´ë¯¸ì§€ ì—…ë¡œë“œ ')
input_image = st.file_uploader(" **ì˜ë¥˜ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”. (ë°°ê²½ì´ ê¹”ë”í•œ ì‚¬ì§„ì´ë¼ë©´ ë” ì¢‹ìŠµë‹ˆë‹¤!)** ", type=['png', 'jpg', 'jpeg'])
if not input_image :
        con = st.container()
        st.stop()
center_image(input_image,400)
st.markdown('--------------------------------------------------------------------------------------')

st.subheader(' âœ… ì—…ë¡œë“œí•œ ì˜ë¥˜ ì´ë¯¸ì§€ ì¹´í…Œê³ ë¦¬ ì„ íƒ ')
input_cat = st.radio(
    "**ê·€í•˜ê°€ ì—…ë¡œë“œí•œ ì˜ë¥˜ ì´ë¯¸ì§€ì˜ ì¹´í…Œê³ ë¦¬ë¥¼ ê³¨ë¼ì£¼ì„¸ìš”.**",
    ['topğŸ‘•', 'bottomğŸ‘–', 'shoesğŸ‘', 'hatğŸ§¢', 'sunglassesğŸ•¶ï¸', 'scarfğŸ§£', 'bagğŸ‘œ'],
    index=None,
    horizontal = True)


if not input_cat :
        con = st.container()
        st.stop()
input_cat = input_cat[:-1]
st.write('You selected:', input_cat)
st.markdown('--------------------------------------------------------------------------------------')

st.subheader(' âœ… ì¶”ì²œë°›ê³  ì‹¶ì€ ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ ì„ íƒ ')
output_cat = st.radio(
    '**ì¶”ì²œë°›ê³  ì‹¶ì€ ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.**',
    ['topğŸ‘•', 'bottomğŸ‘–', 'shoesğŸ‘', 'hatğŸ§¢', 'sunglassesğŸ•¶ï¸', 'scarfğŸ§£', 'bagğŸ‘œ'],
    index=None,
    horizontal = True)

if not output_cat :
        con = st.container()
        st.write('ğŸš« ì£¼ì˜: ì—…ë¡œë“œí•œ ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ì™€ ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.')
        st.stop()
output_cat = output_cat[:-1]
st.write('You selected:', output_cat)
st.write(' ')
st.markdown('--------------------------------------------------------------------------------------')


st.subheader(' âœ… ìƒí™© ì¹´í…Œê³ ë¦¬ ì„ íƒ ')
situation = st.radio(
    "**ìƒí™© ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.**",
    ['ì—¬í–‰ğŸŒŠ', 'ì¹´í˜â˜•ï¸', 'ì „ì‹œíšŒğŸ–¼ï¸', 'ìº í¼ìŠ¤ğŸ« & ì¶œê·¼ğŸ’¼', 'ê¸‰ì¶”ìœ„ğŸ¤§', 'ìš´ë™ğŸ’ª'],
    captions = ['(ë°”ë‹¤,ì—¬í–‰)','(ì¹´í˜, ë°ì¼ë¦¬)','(ë°ì´íŠ¸, ê²°í˜¼ì‹)','','',''],
    index=None,
    horizontal = True)

# ì„ íƒëœ ìƒí™© ì¹´í…Œê³ ë¦¬ë¥¼ ì˜ì–´ë¡œ ë³€í™˜í•´ì„œ ë³€ìˆ˜ ì €ì¥
situation_mapping = {
    'ì—¬í–‰ğŸŒŠ': 'travel',
    'ì¹´í˜â˜•ï¸': 'cafe',
    'ì „ì‹œíšŒğŸ–¼ï¸': 'exhibit',
    'ìº í¼ìŠ¤ğŸ« & ì¶œê·¼ğŸ’¼': 'campus_work',
    'ê¸‰ì¶”ìœ„ğŸ¤§': 'cold',
    'ìš´ë™ğŸ’ª': 'exercise'}

if not situation:
        con = st.container()
        st.stop()
situation= situation_mapping[situation]
st.write('You selected:', situation)

## ë³€ìˆ˜ ëª…
# input_img
# input_cat : ì…ì€ ì˜· ì¹´í…Œê³ ë¦¬
# output_cat  : ì¶”ì²œ ë°›ì„ ì¹´í…Œê³ ë¦¬
# situation : ìƒí™©

st.markdown('--------------------------------------------------------------------------------------')


### ì…ë ¥ë°›ì€ ì´ë¯¸ì§€ segmentation & detection & vectorë³€í™˜ ###
image = Image.open(input_image)

# object detection & cropping í•¨ìˆ˜
def cropping(images,st = 1,
  fi = 0.0,
  step = -0.05):
  image_1 = Image.fromarray(images)
  inputs = processor_obj(images=image_1, return_tensors="pt")
  outputs = model_obj(**inputs)
  for tre in np.arange(st,fi,step):
    try:
        # convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)
        target_sizes = torch.tensor([image_1.size[::-1]])
        results = processor_obj.post_process_object_detection(outputs, threshold=tre, target_sizes=target_sizes)[0]
        
        img = None
        for idx, (score, label, box) in enumerate(zip(results["scores"], results["labels"], results["boxes"])):
            box = [round(i, 2) for i in box.tolist()]
            xmin, ymin, xmax, ymax = box
            img = image_1.crop((xmin, ymin, xmax, ymax))

        poss = np.array(img).sum().sum()
        return img
        break
    except:
        continue
  return images

# vector ë³€í™˜ í•¨ìˆ˜
default_path = './'
def image_to_vector(image,resize_size=(256,256)):  # ì´ë¯¸ì§€ size ë³€í™˜ resize(256,256)
    #image = Image.fromarray(image)
    #image = image.resize(resize_size)
    image = Image.fromarray(np.copy(image))
    image = image.resize(resize_size)
    image_array = np.array(image, dtype=np.float32)
    image_vector = image_array.flatten()
    return image_vector

# ì „ì²´ í†µí•© í•¨ìˆ˜
def final_image(image):    
    if len(np.array(image).shape) == 2:
        image = Image.fromarray(image).convert('RGB')
    # segmentation
    inputs = processor_seg(images=image, return_tensors="pt")
    outputs = model_seg(**inputs)
    logits = outputs.logits.cpu()
    upsampled_logits = nn.functional.interpolate(
            logits,
            size=image.size[::-1],
            mode="bilinear",
            align_corners=False,
        )
    pred_seg = upsampled_logits.argmax(dim=1)[0]
    segments = torch.unique(pred_seg)
    default_path = './'
    
    for i in segments:
        if int(i) == 0:
            continue
        if int(i) == 1:
            cloth = 'hat'
            cloths = 'hat'
            mask = pred_seg == i
            image = np.array(image)
            mask_np = (mask * 255).numpy().astype(np.uint8)
            result = cv2.bitwise_and(image.astype(np.uint8), image.astype(np.uint8), mask=mask_np)
            img = cropping(result)
            img_vector = image_to_vector(img)
        elif int(i) == 3:
            cloth= 'sunglasses'
            cloths= 'sunglasses'
            mask = pred_seg == i
            image = np.array(image)
            mask_np = (mask * 255).numpy().astype(np.uint8)
            result = cv2.bitwise_and(image.astype(np.uint8), image.astype(np.uint8), mask=mask_np)
            img = cropping(result)
            img_vector = image_to_vector(img)
        elif int(i) == 4:
            cloth = 'top'
            cloths = 'top'
            mask = pred_seg == i
            image = np.array(image)
            mask_np = (mask * 255).numpy().astype(np.uint8)
            result = cv2.bitwise_and(image.astype(np.uint8), image.astype(np.uint8), mask=mask_np)
            img = cropping(result)
            img_vector = image_to_vector(img)
        elif int(i) in [5,6,7]:
            cloth= ['pants','skirt','dress']
            cloths= 'bottom'
            mask  = (pred_seg == torch.tensor(5)) | (pred_seg == torch.tensor(6)) | (pred_seg == torch.tensor(7))
            image = np.array(image)
            mask_np = (mask * 255).numpy().astype(np.uint8)
            result = cv2.bitwise_and(image.astype(np.uint8), image.astype(np.uint8), mask=mask_np)
            img = cropping(result)
            img_vector = image_to_vector(img)
        elif int(i) == 8:
            cloth = 'belt'
            cloths = 'belt'
            mask = pred_seg == torch.tensor(8)
            image = np.array(image)
            mask_np = (mask * 255).numpy().astype(np.uint8)
            result = cv2.bitwise_and(image.astype(np.uint8), image.astype(np.uint8), mask=mask_np)
            img = cropping(result)
            img_vector = image_to_vector(img)
        elif (int(i) == 9):
            cloth = 'shoes'
            cloths = 'shoes'
            mask = (pred_seg == torch.tensor(9)) | (pred_seg == torch.tensor(10))
            image = np.array(image)
            mask_np = (mask * 255).numpy().astype(np.uint8)
            result = cv2.bitwise_and(image.astype(np.uint8), image.astype(np.uint8), mask=mask_np)
            img = cropping(result)
            img_vector = image_to_vector(img)
        elif int(i) == 16:
            cloth = 'bag'
            cloths = 'bag'
            mask = pred_seg == torch.tensor(16)
            image = np.array(image)
            mask_np = (mask * 255).numpy().astype(np.uint8)
            result = cv2.bitwise_and(image.astype(np.uint8), image.astype(np.uint8), mask=mask_np)
            img = cropping(result)
            img_vector = image_to_vector(img)
        elif int(i) == 17:
            cloth = 'scarf'
            cloths = 'scarf'
            mask = pred_seg == torch.tensor(17)
            image = np.array(image)
            mask_np = (mask * 255).numpy().astype(np.uint8)
            result = cv2.bitwise_and(image.astype(np.uint8), image.astype(np.uint8), mask=mask_np)
            img = cropping(result)
            img_vector = image_to_vector(img)
        return img_vector
    
# ì…ë ¥ë°›ì€ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì™„ë£Œ
input_img = final_image(image)

### ìœ ì‚¬ë„ ë¶„ì„ ###
# í•˜ë‚˜ëŠ” ì´ë¯¸ì§€, ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ê²½ë¡œë¡œ ë°›ëŠ” ê²½ìš°
def cosine_similarity(vec1, vec2_path):
    vec2 = np.loadtxt(vec2_path)
    dot_product = np.dot(vec1, vec2)
    norm_vec1 = np.linalg.norm(vec1)
    norm_vec2 = np.linalg.norm(vec2)
    similarity = dot_product / (norm_vec1 * norm_vec2)   
    return similarity

# ë‘˜ ë‹¤ ê²½ë¡œë¡œ ë°›ëŠ” ê²½ìš°
def cosine_similarity_2(vec1_path, vec2_path):
    vec1 = np.loadtxt(vec1_path)
    vec2 = np.loadtxt(vec2_path)
    dot_product = np.dot(vec1, vec2)
    norm_vec1 = np.linalg.norm(vec1)
    norm_vec2 = np.linalg.norm(vec2)
    similarity = dot_product / (norm_vec1 * norm_vec2)
    return similarity

with st.spinner('Wait for it...'):
    # ì…ë ¥ë°›ì€ ì´ë¯¸ì§€ & ë™ì¼ ì¹´í…Œê³ ë¦¬ í´ë”ì— ì €ì¥ëœ ìŠ¤íƒ€ì¼ ì´ë¯¸ì§€
    sim_list = []
    file_path = './style/' + situation + '/' + input_cat + '/'   # ex) './cafe/top/'
    cloths = os.listdir('./style/' + situation + '/' + input_cat + '/')
    for cloth in cloths:
        sim_list.append(cosine_similarity(input_img, file_path + cloth))
    max_idx = np.argmax(sim_list)

    # target_image ì •ì˜
    target_image = './style/' + situation + '/' + output_cat + '/' + cloths[max_idx]
    # ìœ ì‚¬ë„ ë¶„ì„ ì™„ë£Œëœ ìŠ¤íƒ€ì¼seg ì´ë¯¸ì§€ì™€ product_seg ìœ ì‚¬ë„ë¶„ì„
    sim_list = []
    file_path = './product/' + output_cat + '/'
    cloths = os.listdir('./product/' + output_cat + '/')
    for cloth in cloths:
        sim_list.append(cosine_similarity_2(target_image, file_path + cloth))
    max_idx = np.argmax(sim_list)
    output_name  = cloths[max_idx]
    ## ì˜ˆì‹œ ì¶œë ¥ê°’: 'bottom_1883.txt'

    # name ë¡œë“œ
    acc_name = pd.read_csv('acc_name.csv')
    bottom_name =pd.read_csv('bottom_name.csv')
    outer_name =pd.read_csv('outer_name.csv')
    shoes_name =pd.read_csv('shoes_name.csv')
    top_name =pd.read_csv('top_name.csv')

    #ìƒí’ˆ ë°ì´í„° ë¡œë“œ
    outer = pd.read_csv('outer.csv')
    top = pd.read_csv('top.csv')
    bottom = pd.read_csv('bottom.csv')
    shoes = pd.read_csv('shoes.csv')
    acc = pd.read_csv('acc.csv')

    if output_cat == 'bottom':
        df = bottom.copy()
        df_name = bottom_name.copy()
    elif output_cat == 'top':
        df = top.copy()
        df_name = top_name.copy()
    elif output_cat == 'shoes':
        df = shoes.copy()
        df_name = shoes_name.copy()
    elif (output_cat == 'hat') or (output_cat == 'sunglasses') or (output_cat == 'scarf') or (output_cat == 'bag') or (output_cat == 'belt'):
        df = acc.copy()
        df_name = acc_name.copy()

    output_name = output_name.split('.')[0]
    file_name = df_name[df_name['index']==output_name].iloc[0,1] #3049906_16754112975667_500.jpg
    final = df[df['id'] == file_name]

    name = final['name'].values[0].split('\n')[-1] # ìƒí’ˆëª…
    price = final['price'].values[0] # ìƒí’ˆê°€ê²©

image_path = './product/img/'

st.subheader('OUTPUT')

img = Image.open(image_path+output_cat+'/'+file_name)

col1, col2, col3 = st.columns(3)
with col1:
    st.image(img,width=400)
with col3:
    st.caption('ìƒí’ëª… : ' + name)
    st.caption('ê°€ê²© : ' + price)
